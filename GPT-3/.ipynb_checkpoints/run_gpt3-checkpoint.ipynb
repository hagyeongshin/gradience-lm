{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "openai.organization = #insert yours\n",
    "openai.api_key = #insert yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_output_gpt3(experiment_item):\n",
    "    \"\"\"Print out the whole output entry.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inserted_prob_gpt3(experiment_item):\n",
    "    \"\"\"Print out the log probability for the inserted tokens.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "    \n",
    "    toplogprob = output.to_dict()['choices'][0].to_dict()['logprobs'].to_dict()[\"top_logprobs\"]\n",
    "    \n",
    "    response = []\n",
    "    \n",
    "    for i in range(0,len(toplogprob)):\n",
    "        response.append(toplogprob[i].to_dict())\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(q):\n",
    "    entropy = -(q * np.log2(q) + (1-q) * np.log2(1-q))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPT-3 uses BPE and unknown characters (usually non-English languages, special characters, etc.) appear opaque.\n",
    "#This function extracts inserted words by grabbing the BPE tokens, then calculates the entropy of the inserted token. \n",
    "\n",
    "def token_entropy_gpt3(toplogprob):\n",
    "    \n",
    "    token_prob = []\n",
    "    \n",
    "    i = 0 \n",
    "    while i < len(toplogprob):\n",
    "        if r'.' in toplogprob[i].keys():\n",
    "            break    \n",
    "            i +=1\n",
    "        else:\n",
    "            token_prob.append(*toplogprob[i].values())\n",
    "            i +=1\n",
    "    \n",
    "    token_entropy = []\n",
    "    \n",
    "    for logprob in token_prob:\n",
    "        token_entropy.append(entropy(np.e**(logprob)))\n",
    "    \n",
    "    return sum(token_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inserted_text_gpt3(experiment_item):\n",
    "    \"\"\"Print out the inserted tokens in a decoded text format.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "    \n",
    "    return output.choices[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = inserted_prob_gpt3(\"민수가 라면을 먹었다. 소희가 라면을 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bytes:\\\\xeb': -0.012674714},\n",
       " {'bytes:\\\\xa8': -0.010898419},\n",
       " {'bytes:\\\\xb9': -4.573365e-05},\n",
       " {'bytes:\\\\xec': -0.021624932},\n",
       " {'bytes:\\\\x97': -0.06579405},\n",
       " {'bytes:\\\\x88': -0.00019869342},\n",
       " {'bytes:\\\\xeb\\\\x8b': -0.029567312},\n",
       " {'bytes:\\\\xa4': -4.277735e-07},\n",
       " {'.': -0.027864005},\n",
       " {'\\n': -0.019333899},\n",
       " {'Min': -0.8997774},\n",
       " {'-': -0.4776559},\n",
       " {'su': -0.48117903},\n",
       " {' ate': -0.046376396},\n",
       " {' ram': -0.18633811}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684050250428971"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entropy_gpt3(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(filename):\n",
    "    \"\"\"Run GPT-3 on experiment items.\"\"\"\n",
    "    \n",
    "    data_path = \"{FILE}.csv\".format(FILE=filename)\n",
    "    df_output = pd.read_csv(data_path)\n",
    "    \n",
    "    prob = []\n",
    "    text = []\n",
    "    cond = []\n",
    "    item = []\n",
    "    entropy = []\n",
    "\n",
    "    with tqdm(total=df_output.shape[0]) as pbar:   \n",
    "        for index, row in df_output.iterrows():\n",
    "        \n",
    "            condition = row['Condition name']\n",
    "            cond.append(condition)\n",
    "        \n",
    "            itemindex = row['Lexicalization']\n",
    "            item.append(itemindex)\n",
    "        \n",
    "            sentence = row['Item']\n",
    "            p = inserted_prob_gpt3(sentence)\n",
    "            prob.append(p)\n",
    "    \n",
    "            t = inserted_text_gpt3(sentence)\n",
    "            text.append(t)\n",
    "            \n",
    "            ent = token_entropy_gpt3(p)\n",
    "            entropy.append(ent)\n",
    "        \n",
    "            pbar.update(1)\n",
    "        \n",
    "    df_output['log_prob'] = prob\n",
    "    df_output['response'] = text\n",
    "    df_output['entropy'] = entropy\n",
    "    \n",
    "    df_output.to_csv(\"{TASK}_output.csv\".format(TASK=filename))\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [00:06<00:01,  1.57s/it]/var/folders/1q/wwq32jk570bfqd71tkq974wr0000gn/T/ipykernel_79807/2651921724.py:2: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy = -(q * np.log2(q) + (1-q) * np.log2(1-q))\n",
      "/var/folders/1q/wwq32jk570bfqd71tkq974wr0000gn/T/ipykernel_79807/2651921724.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  entropy = -(q * np.log2(q) + (1-q) * np.log2(1-q))\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:08<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "sample = main(\"sample_gpt3_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bytes:\\\\xeb': -0.706179},\n",
       " {'bytes:\\\\x81': -1.0260508},\n",
       " {'bytes:\\\\x84': -0.3326238},\n",
       " {'bytes:\\\\xeb': -0.8584608},\n",
       " {'bytes:\\\\x8a': -0.45750463},\n",
       " {'bytes:\\\\x94': -0.0057951147},\n",
       " {'bytes: \\\\xeb': -0.9150861},\n",
       " {'bytes:\\\\xb0': -0.2612375},\n",
       " {'bytes:\\\\xa9': -0.00036503928},\n",
       " {'bytes:\\\\xeb': -0.0015714729},\n",
       " {'bytes:\\\\xb2': -0.00023535996},\n",
       " {'bytes:\\\\x95': -0.00033534507},\n",
       " {'\\n': -0.26113656},\n",
       " {'\\n': -0.020923948},\n",
       " {'1': -0.6555963},\n",
       " {'.': -0.009052616}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['log_prob'][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
