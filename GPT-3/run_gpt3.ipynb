{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "openai.organization = \"org-yXczeVepZFhzQLUizzrJHIdC\" #insert yours\n",
    "openai.api_key = \"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\" #insert yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_output_gpt3(experiment_item):\n",
    "    \"\"\"Print out the whole output entry.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inserted_prob_gpt3(experiment_item):\n",
    "    \"\"\"Print out the log probability for the inserted tokens.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "    \n",
    "    toplogprob = output.to_dict()['choices'][0].to_dict()['logprobs'].to_dict()[\"top_logprobs\"]\n",
    "    \n",
    "    response = []\n",
    "    \n",
    "    for i in range(0,len(toplogprob)):\n",
    "        response.append(toplogprob[i].to_dict())\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def entropy(q):\n",
    "    #entropy = -(q * np.log2(q))\n",
    "#    entropy = -(q * np.log2(q) + (1-q) * np.log2(1-q))\n",
    "#    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPT-3 uses BPE and unknown characters (usually non-English languages, special characters, etc.) appear opaque.\n",
    "#This function extracts inserted words by grabbing the BPE tokens, then calculates the entropy of the inserted token. \n",
    "\n",
    "def token_entropy_gpt3(toplogprob):\n",
    "    \n",
    "    token_prob = []\n",
    "    \n",
    "    i = 0 \n",
    "    while i < len(toplogprob):\n",
    "        if r'.' in toplogprob[i].keys():\n",
    "            break    \n",
    "            i +=1\n",
    "        else:\n",
    "            token_prob.append(*toplogprob[i].values())\n",
    "            i +=1\n",
    "    \n",
    "    token_entropy = []\n",
    "    \n",
    "    for logprob in token_prob:\n",
    "        token_entropy.append(entropy(np.e**(logprob)))\n",
    "    \n",
    "    return sum(token_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting mean unlogged probablities aggregated for inserted token.\n",
    "\n",
    "def token_prob_gpt3(toplogprob):\n",
    "    \n",
    "    token = []\n",
    "    \n",
    "    i = 0 \n",
    "    while i < len(toplogprob):\n",
    "        if r'.' in toplogprob[i].keys():\n",
    "            break    \n",
    "            i +=1\n",
    "        else:\n",
    "            token.append(*toplogprob[i].values())\n",
    "            i +=1\n",
    "    \n",
    "    token_prob = []\n",
    "    \n",
    "    for i in token:\n",
    "        token_prob.append(np.e**(i))\n",
    "        \n",
    "    return sum(token_prob)/len(token_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_surprisal_gpt3(toplogprob):\n",
    "    \n",
    "    token_prob = []\n",
    "    \n",
    "    i = 0 \n",
    "    while i < len(toplogprob):\n",
    "        if r'.' in toplogprob[i].keys():\n",
    "            break    \n",
    "            i +=1\n",
    "        else:\n",
    "            token_prob.append(*toplogprob[i].values())\n",
    "            i +=1\n",
    "    \n",
    "    token_surprisal = []\n",
    "    \n",
    "    for logprob in token_prob:\n",
    "        token_surprisal.append(-np.log2(np.e**(logprob)))\n",
    "    \n",
    "    return sum(token_surprisal)/len(token_surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inserted_text_gpt3(experiment_item):\n",
    "    \"\"\"Print out the inserted tokens in a decoded text format.\"\"\"\n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        top_p= 1,\n",
    "        n= 1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream= False,\n",
    "        logprobs= 1,\n",
    "        stop= \"\\n\"\n",
    "        )\n",
    "    \n",
    "    return output.choices[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = inserted_prob_gpt3(\"민수가 라면을 먹었다. 소희가 라면을 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bytes:\\\\xeb': -0.012631072},\n",
       " {'bytes:\\\\xa8': -0.012214357},\n",
       " {'bytes:\\\\xb9': -4.5255874e-05},\n",
       " {'bytes:\\\\xec': -0.022307266},\n",
       " {'bytes:\\\\x97': -0.061792746},\n",
       " {'bytes:\\\\x88': -0.00019774071},\n",
       " {'bytes:\\\\xeb\\\\x8b': -0.029641602},\n",
       " {'bytes:\\\\xa4': -1.5006569e-06},\n",
       " {'.': -0.02828858},\n",
       " {'\\n': -0.019064488},\n",
       " {'Min': -0.6241813},\n",
       " {'-': -0.47174278}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02503644682806538"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_surprisal_gpt3(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829843285417406"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_prob_gpt3(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(filename):\n",
    "    \"\"\"Run GPT-3 on experiment items.\"\"\"\n",
    "    \n",
    "    data_path = \"{FILE}.csv\".format(FILE=filename)\n",
    "    df_output = pd.read_csv(data_path)\n",
    "    \n",
    "    logprob = []\n",
    "    prob = []\n",
    "    text = []\n",
    "    cond = []\n",
    "    item = []\n",
    "    surprisal = []\n",
    "\n",
    "    with tqdm(total=df_output.shape[0]) as pbar:   \n",
    "        for index, row in df_output.iterrows():\n",
    "        \n",
    "            condition = row['Condition name']\n",
    "            cond.append(condition)\n",
    "        \n",
    "            itemindex = row['Lexicalization']\n",
    "            item.append(itemindex)\n",
    "        \n",
    "            sentence = row['Item']\n",
    "            logp = inserted_prob_gpt3(sentence)\n",
    "            logprob.append(logp)\n",
    "    \n",
    "            t = inserted_text_gpt3(sentence)\n",
    "            text.append(t)\n",
    "            \n",
    "            surp = token_surprisal_gpt3(logp)\n",
    "            surprisal.append(surp)\n",
    "            \n",
    "            tokenpr = token_prob_gpt3(logp)\n",
    "            prob.append(tokenpr)\n",
    "        \n",
    "            pbar.update(1)\n",
    "        \n",
    "    df_output['text'] = text\n",
    "    df_output['text_prob'] = prob\n",
    "    df_output['token_prob'] = logprob\n",
    "    df_output['text_surprisal'] = surprisal\n",
    "    \n",
    "    df_output.to_csv(\"{TASK}_output.csv\".format(TASK=filename))\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "sample = main(\"sample_gpt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idx</th>\n",
       "      <th>Lexicalization</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Condition name</th>\n",
       "      <th>Item</th>\n",
       "      <th>text</th>\n",
       "      <th>text_prob</th>\n",
       "      <th>token_prob</th>\n",
       "      <th>text_surprisal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>nc-nom</td>\n",
       "      <td>지은이 사실을</td>\n",
       "      <td>알고 있습니다.</td>\n",
       "      <td>0.833589</td>\n",
       "      <td>[{'bytes:\\xec': -0.9494628}, {'bytes:\\x95': -0...</td>\n",
       "      <td>0.318662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>nc-nom</td>\n",
       "      <td>호준이 용돈을</td>\n",
       "      <td>받았습니다.</td>\n",
       "      <td>0.893281</td>\n",
       "      <td>[{'bytes:\\xeb': -0.5006832}, {'bytes:\\xb0': -0...</td>\n",
       "      <td>0.188362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>nc-nom</td>\n",
       "      <td>은지가 불을</td>\n",
       "      <td>끄는 것을 도와</td>\n",
       "      <td>0.670222</td>\n",
       "      <td>[{'bytes:\\xeb': -0.69370914}, {'bytes:\\x81': -...</td>\n",
       "      <td>0.669550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>nc-nom</td>\n",
       "      <td>용준이 마중을</td>\n",
       "      <td>나갔어요.</td>\n",
       "      <td>0.843312</td>\n",
       "      <td>[{'bytes:\\xeb': -0.90277714}, {'bytes:\\x82': -...</td>\n",
       "      <td>0.297424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>nc-nom</td>\n",
       "      <td>은영이 저녁을</td>\n",
       "      <td>먹고 있습니다.</td>\n",
       "      <td>0.806992</td>\n",
       "      <td>[{'bytes:\\xeb': -0.68625844}, {'bytes:\\xa8': -...</td>\n",
       "      <td>0.355033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idx  Lexicalization  Condition Condition name      Item      text  \\\n",
       "0    0               1          1         nc-nom  지은이 사실을   알고 있습니다.   \n",
       "1    1               2          1         nc-nom  호준이 용돈을     받았습니다.   \n",
       "2    2               3          1         nc-nom   은지가 불을   끄는 것을 도와   \n",
       "3    3               4          1         nc-nom  용준이 마중을      나갔어요.   \n",
       "4    4               5          1         nc-nom  은영이 저녁을   먹고 있습니다.   \n",
       "\n",
       "   text_prob                                         token_prob  \\\n",
       "0   0.833589  [{'bytes:\\xec': -0.9494628}, {'bytes:\\x95': -0...   \n",
       "1   0.893281  [{'bytes:\\xeb': -0.5006832}, {'bytes:\\xb0': -0...   \n",
       "2   0.670222  [{'bytes:\\xeb': -0.69370914}, {'bytes:\\x81': -...   \n",
       "3   0.843312  [{'bytes:\\xeb': -0.90277714}, {'bytes:\\x82': -...   \n",
       "4   0.806992  [{'bytes:\\xeb': -0.68625844}, {'bytes:\\xa8': -...   \n",
       "\n",
       "   text_surprisal  \n",
       "0        0.318662  \n",
       "1        0.188362  \n",
       "2        0.669550  \n",
       "3        0.297424  \n",
       "4        0.355033  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('알', 'VV'), ('고', 'ECE'), ('있', 'VXV'), ('습니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(sample['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do pos tagging for all text responses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
